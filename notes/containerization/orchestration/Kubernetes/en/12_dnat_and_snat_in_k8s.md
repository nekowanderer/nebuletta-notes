# DNAT and SNAT in K8S

[English](../en/12_dnat_and_snat_in_k8s.md) | [繁體中文](../zh-tw/12_dnat_and_snat_in_k8s.md) | [日本語](../ja/12_dnat_and_snat_in_k8s.md) | [Back to Index](../README.md)

### DNAT
Destination Network Address Translation
- Rewrites the destination IP/Port of packets to another address.
- Traditional firewall/NAT router: Often converts external 203.0.113.10:80 to internal 192.168.1.100:80.
- Kubernetes: Converts traffic hitting nodeIP:NodePort or ClusterIP:port to the actual PodIP:containerPort.

```
  External Client
          |
          v
203.0.113.10:32001  (NodePort)
          |
          v
    [DNAT Generated]
          |
          v
10.244.3.42:8080  (Pod)

```

##### Why DNAT?
- Abstraction: Service / NodePort / LoadBalancer provides "persistent addresses"; real Pod IP changes during reconstruction are acceptable.
- Load balancing: Same entry address can round-robin to multiple backend Pods.

##### How is DNAT implemented in K8s?
- kube-proxy (iptables/IPVS)
  - Injects -j KUBE-SVC-XXXX in the nat table's PREROUTING (incoming) or OUTPUT (locally generated packets) chains.
  - KUBE-SVC-XXXX selects one -j KUBE-SEP-YYYY (each SEP represents a Pod).
  - KUBE-SEP-YYYY finally performs -j DNAT --to-destination PodIP:Port.
- eBPF/XDP (Cilium, Calico eBPF)
  - Uses kernel BPF programs to rewrite at earlier hooks (TC / XDP) for better performance.

### SNAT
Source Network Address Translation
- Rewrites the source IP/Port of packets to another address ── opposite to DNAT which changes the "destination"
- Ensures return packets can "find their way back" or consolidates multiple internal addresses into one external address
- Common terms: SNAT, MASQUERADE (dynamic SNAT), NAPT (simultaneously changing IP and Port)

##### Practical scenarios in Kubernetes
- Pod accessing internet (Pod → Internet)
  - Pod addresses are usually 10.x/172.x/192.168.x private network segments, not recognized by public internet routing.
  - Worker Node performs SNAT --to-source nodeIP or MASQUERADE in nat POSTROUTING chain, changing source to node's public IP.
  - External return packets only see node IP, sent back to node then found corresponding Pod via conntrack.
- Service type=NodePort / LoadBalancer return flow
  - If ExternalTrafficPolicy=Cluster (default), node DNATs external traffic to Pod then SNATs source → Pod only sees node IP.
  - Benefit: Pod return packets only need to go back to node; drawback: loses original Client IP.
- Cross-Node Pod to Pod (some CNI Plugins)
  - If CNI uses tunnels or overlays, may SNAT again at node exit, ensuring stable return path.

##### What do iptables rules look like?
```bash
# Example generated by kube-proxy
-A POSTROUTING -s 10.244.0.0/16 ! -d 10.244.0.0/16 \
        -j MASQUERADE        # Changes source of outgoing Pod traffic to Node IP
```
- POSTROUTING: Last moment before packets leave the local machine.
- MASQUERADE: Dynamically selects node's current external IP (suitable for cloud nodes that may change IP).
- Can disable this behavior with ip-masq-agent or CNI's --masq-outbound=false.

### DNAT v.s. SNAT
| Name                    | Purpose               | When to Use               | Common Execution Location |
| --------------------- | ---------------- | ------------------- | ------------------- |
| **DNAT**              | Change destination IP/Port | Direct traffic to backend Pods         | `PREROUTING` / `OUTPUT` |
| **SNAT / MASQUERADE** | Change source IP/Port | When Pods go external, ensure return packets can reach node | `POSTROUTING`           |

### Common Questions
- Can real Client IP be preserved?
  - Set Service to ExternalTrafficPolicy=Local to avoid SNAT; but only works when traffic lands on nodes "with corresponding Pods locally".
  - Ingress Controllers (Nginx, Traefik) usually read X-Forwarded-For to preserve original IP.
- Will SNAT become a bottleneck?
  - With many connections, conntrack table needs sufficient memory; eBPF/XDP solutions can reduce iptables jump costs.
  - Cloud NAT Gateway (AWS GWLB, GCP Cloud NAT) can offload Node SNAT burden.
- What about on-prem/bare metal clusters?
  - Can use MetalLB + additional firewall; or BGP CNI (Cilium BGP, Calico BGP) to directly advertise Pod CIDR to upstream routing, eliminating SNAT.

### Relationship between NAT/DNAT/SNAT

| Name           | Modified Fields                 | Common Direction            | Representative Scenarios in K8s              |
| ------------ | -------------------- | --------------- | ------------------------- |
| **NAT** (General) | Any source or destination IP/Port      | ——              | Umbrella term             |
| **DNAT**     | **Destination** | "External→Internal"<br>Entry load balancing | `nodeIP:NodePort → PodIP` |
| **SNAT**     | **Source**      | "Internal→External"<br>Exit internet access   | `PodIP → nodeIP`          |

- NAT (Network Address Translation)
  - General term: Any action that changes IP/port in packet headers is called NAT.
  - Subcategories: Further divided into DNAT and SNAT based on "which field to change".
- DNAT (Destination NAT)
  - Key point: Changes destination, allowing packets to reach "new targets".
  - Typical uses
    - Home: Directs home router:80 to NAS:80.
    - K8s: Directs ClusterIP / NodePort to PodIP.
  - iptables often appears in nat PREROUTING (changes immediately when packets come in).
- SNAT (Source NAT) / MASQUERADE
  - Key point: Changes source, ensuring consistent return path, or does many-to-one (Many → One) address conservation.
  - Typical uses
    - Home NAT: Multiple computers share one WAN IP for internet access.
    - K8s: When Pods go external, changes to node IP; external return packets can find their way.
    - NodePort/LoadBalancer with ExternalTrafficPolicy=Cluster covers client IP with node IP.
  - iptables often appears in nat POSTROUTING (final change before packets leave local machine).

##### Three "NAT Pipelines" Applied to Kubernetes

```
(1) External → LoadBalancer (public/private IP)
                   |  DNAT (L4/L7 LB)
(2) → nodeIP:NodePort                ⟵ External clients only see Node
                   |  DNAT (kube-proxy / eBPF)
(3) → PodIP:containerPort            ⟵ Pod only sees Node
                   |  SNAT (as needed) ── if client IP needs to be covered
(4) ← Pod→Node  (if SNAT enabled)         ⟵ Return path via same node

```
- Steps 1–2: Cloud LB performs first DNAT, directing traffic to node's NodePort.
- Steps 2–3: kube-proxy/eBPF performs second DNAT, directing traffic to actual Pod.
- Steps 3–4: To ensure packets can return, node SNATs source to node IP as needed (if preserving client IP, disable SNAT, set ExternalTrafficPolicy=Local and ensure traffic only hits nodes with Pods).

##### Common Questions
- Is NAT = DNAT + SNAT?
  - Almost, but there are also PAT/NAPT (simultaneously changing IP and port), Hairpin NAT, Double NAT, etc., essentially special cases of NAT.
- How to preserve real Client IP?
  - Set K8s Service ExternalTrafficPolicy=Local to prevent node SNAT; drawback is can only forward to nodes "with local Pods".
  - Or add PROXY Protocol, X-Forwarded-For at Ingress/LB layer for application parsing.
- Will DNAT+SNAT affect performance?
  - iptables with many connections has significant conntrack Table and Rule matching costs; switching to eBPF or offloading to cloud NAT Gateway can greatly reduce CPU and latency. 